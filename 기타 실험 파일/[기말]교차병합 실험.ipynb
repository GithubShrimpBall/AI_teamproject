{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8c3f0a-f933-4a88-9414-50a2686ad694",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout, Flatten, Attention, LayerNormalization, Reshape\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "from keras.utils import plot_model\n",
    "from IPython.display import Image\n",
    "\n",
    "# 데이터셋 로드\n",
    "df = pd.read_csv('/content/malicious_phish.csv')\n",
    "\n",
    "# 1. 라벨 인코딩\n",
    "label_encoder = LabelEncoder()\n",
    "df['label_encoded'] = label_encoder.fit_transform(df['type'])\n",
    "y = to_categorical(df['label_encoded'])\n",
    "\n",
    "# 2. 텍스트 데이터 전처리\n",
    "urls = df['url'].values\n",
    "max_sequence_length = 200\n",
    "max_num_words = 10000\n",
    "embedding_dim = 128\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_num_words, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(urls)\n",
    "seq_data = tokenizer.texts_to_sequences(urls)\n",
    "seq_data = pad_sequences(seq_data, maxlen=max_sequence_length)\n",
    "\n",
    "# URL의 type을 매핑\n",
    "df_features = pd.DataFrame()\n",
    "\n",
    "type_mapping = {\n",
    "    'benign': 0,\n",
    "    'phishing': 1,\n",
    "    'defacement': 2,\n",
    "    'malware': 3\n",
    "}\n",
    "df_features['url'] = df['url']\n",
    "# df_features['type'] = df['type'].map(type_mapping)\n",
    "df_features\n",
    "\n",
    "# URL의 길이\n",
    "df_features['url_length'] = df['url'].apply(len)\n",
    "\n",
    "# IP 주소 포함 여부\n",
    "# df_features['has_ip'] = df['url'].str.contains(r'\\d+\\.\\d+\\.\\d+\\.\\d+').astype(int)\n",
    "\n",
    "# URL이 'https://'로 시작하는지 여부\n",
    "# df_features['is_https'] = df['url'].str.startswith('https://').astype(int)\n",
    "\n",
    "# 숫자의 개수\n",
    "# df_features['digit_count'] = df['url'].str.count(r'\\d')\n",
    "\n",
    "# 영문자 개수\n",
    "# df_features['letter_count'] = df['url'].str.count(r'[a-zA-Z]')\n",
    "\n",
    "df_features\n",
    "\n",
    "# IP 주소 포함 여부\n",
    "def has_ip(url):\n",
    "  return int(bool(re.search(r'\\d+\\.\\d+\\.\\d+\\.\\d+', url)))\n",
    "\n",
    "df_features['has_ip'] = df['url'].apply(has_ip)\n",
    "df_features\n",
    "\n",
    "# URL이 'https://'로 시작하는지 여부\n",
    "def is_https(url):\n",
    "  return int(url.startswith('https://'))\n",
    "\n",
    "df_features['is_https'] = df['url'].apply(is_https)\n",
    "df_features\n",
    "\n",
    "# 숫자의 개수\n",
    "def digit_count(url):\n",
    "  return sum(c.isdigit() for c in url)\n",
    "\n",
    "df_features['digit_count'] = df['url'].apply(digit_count)\n",
    "df_features\n",
    "\n",
    "# 영문자의 개수\n",
    "def letter_count(url):\n",
    "  return sum(c.isalpha() for c in url)\n",
    "\n",
    "df_features['letter_count'] = df['url'].apply(letter_count)\n",
    "df_features\n",
    "\n",
    "# 찾고자 하는 TLD 목록\n",
    "tld_list = ['.tk', '.buzz', '.xyz', '.top', '.ga', '.ml', '.info', '.cf', '.gq',\n",
    "            '.icu', '.wang', '.live', '.host', '.shop', '.vip', '.id', '.cc',\n",
    "            '.br', '.ci', '.zw', '.sx', '.mw']\n",
    "\n",
    "# TLD 개수\n",
    "df_features['tld_count'] = df['url'].apply(lambda x: sum(x.endswith(tld) for tld in tld_list))\n",
    "\n",
    "df_features\n",
    "\n",
    "def count_special_chars(url):\n",
    "    non_alpha_num = re.findall(r'\\W',url)\n",
    "    return len(non_alpha_num)\n",
    "\n",
    "df_features['special_chars_count'] = df['url'].apply(count_special_chars)   # 특수 문자의 갯수\n",
    "df_features\n",
    "\n",
    "def count_keyword_flags(url):\n",
    "    # 찾고자 하는 키워드 목록\n",
    "    keywords = [\n",
    "        \"php\", \"index\", \"option\",\"article\", \"content\", \"html\", \"view\", \"component\"\n",
    "    ]\n",
    "\n",
    "    # URL을 소문자로 변환\n",
    "    url_lower = url.lower()\n",
    "\n",
    "    # 각 키워드가 포함되어 있는지 여부를 1 또는 0으로 반환\n",
    "    return {keyword: (1 if keyword in url_lower else 0) for keyword in keywords}\n",
    "\n",
    "# 각 URL에 대해 키워드 포함 여부를 데이터프레임으로 변환\n",
    "keyword_flags_df = df['url'].apply(count_keyword_flags).apply(pd.Series)\n",
    "\n",
    "# 기존 데이터프레임에 키워드 포함 여부 데이터프레임을 병합\n",
    "df_features = pd.concat([df_features, keyword_flags_df], axis=1)\n",
    "\n",
    "df_features\n",
    "\n",
    "def count_slash(url):\n",
    "    # '/'의 개수 계산\n",
    "    return url.count('/')\n",
    "\n",
    "def count_hyphen(url):\n",
    "    # '-'의 개수 계산\n",
    "    return url.count('-')\n",
    "\n",
    "def count_dot(url):\n",
    "    # '.'의 개수 계산\n",
    "    return url.count('.')\n",
    "\n",
    "def count_underscore(url):\n",
    "    # '_'의 개수 계산\n",
    "    return url.count('_')\n",
    "\n",
    "def count_equals(url):\n",
    "    # '='의 개수 계산\n",
    "    return url.count('=')\n",
    "\n",
    "def count_question(url):\n",
    "    # '_'의 개수 계산\n",
    "    return url.count('?')\n",
    "\n",
    "def count_percent(url):\n",
    "    # '%'의 개수 계산\n",
    "    return url.count('%')\n",
    "\n",
    "df_features['slash_count'] = df['url'].apply(lambda x: count_slash(x))\n",
    "df_features['hyphen_count'] = df['url'].apply(lambda x: count_hyphen(x))\n",
    "df_features['underscore_count'] = df['url'].apply(lambda x: count_underscore(x))\n",
    "df_features['dot_count'] = df['url'].apply(lambda x: count_dot(x))\n",
    "df_features['equals_count'] = df['url'].apply(lambda x: count_equals(x))\n",
    "df_features['question_count'] = df['url'].apply(lambda x: count_question(x))\n",
    "df_features['percent_count'] = df['url'].apply(lambda x: count_percent(x))\n",
    "\n",
    "# 결과 확인\n",
    "df_features\n",
    "\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "\n",
    "def count_url_parameters(url):\n",
    "    # URL 파싱\n",
    "    parsed_url = urlparse(url)\n",
    "\n",
    "    # URL로부터 query 추출\n",
    "    query = parsed_url.query\n",
    "\n",
    "    # parse_qs() 이용하여 query 분석\n",
    "    parameters = parse_qs(query)\n",
    "\n",
    "    # parameter 개수 리턴\n",
    "    return len(parameters)\n",
    "\n",
    "df_features['param_count'] = df['url'].apply(count_url_parameters)    # URL의 parameter 개수\n",
    "df_features\n",
    "\n",
    "def no_of_dir(url):\n",
    "    urldir = urlparse(url).path\n",
    "    return urldir.count('/')\n",
    "\n",
    "df_features['count_dir'] = df['url'].apply(lambda i: no_of_dir(i))    # directory 개수\n",
    "df_features\n",
    "\n",
    "def abnormal_url(url):\n",
    "    hostname = urlparse(url).hostname\n",
    "    hostname = str(hostname)\n",
    "    match = re.search(hostname, url)\n",
    "    if match:\n",
    "        # print match.group()\n",
    "        return 1\n",
    "    else:\n",
    "        # print 'No matching pattern found', \"component\"\n",
    "        return 0\n",
    "\n",
    "\n",
    "df_features['abnormal_url'] = df['url'].apply(lambda i: abnormal_url(i))\n",
    "df_features\n",
    "\n",
    "# 'www' 갯수\n",
    "df_features['count_www'] = df['url'].apply(lambda i: i.count('www'))\n",
    "df_features\n",
    "\n",
    "def suspicious_words(url):\n",
    "    match = re.search('PayPal|login|signin|bank|account|update|free|lucky|service|bonus|ebayisapi|webscr',\n",
    "                      url)\n",
    "    if match:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "df_features['sus_url'] = df['url'].apply(lambda i: suspicious_words(i))\n",
    "df_features\n",
    "\n",
    "# 열 이름 출력\n",
    "print(\"x_val 배열의 열 이름 (순서대로):\")\n",
    "for idx, name in enumerate(df_features.columns[1:]): # url 열 제외\n",
    "    print(f\"Feature_{idx}: {name}\")\n",
    "\n",
    "# 4. 수치형 특성과 시퀀스 데이터 결합\n",
    "# Select only numerical features from df_features\n",
    "numerical_features = df_features.select_dtypes(include=np.number).columns\n",
    "X_num = df_features[numerical_features].values\n",
    "\n",
    "X_seq = seq_data\n",
    "\n",
    "# Convert X_num to float32\n",
    "X_num = X_num.astype(np.float32) # Convert X_num to float32\n",
    "\n",
    "# 데이터 나누기\n",
    "X_train_num, X_temp_num, X_train_seq, X_temp_seq, y_train, y_temp = train_test_split(\n",
    "    X_num, X_seq, y, test_size=0.4, random_state=42, stratify=y.argmax(axis=1)\n",
    ")\n",
    "X_val_num, X_test_num, X_val_seq, X_test_seq, y_val, y_test = train_test_split(\n",
    "    X_temp_num, X_temp_seq, y_temp, test_size=0.5, random_state=42, stratify=y_temp.argmax(axis=1)\n",
    ")\n",
    "\n",
    "# 5. 모델 구성\n",
    "# 수치형 입력\n",
    "input_num = Input(shape=(X_train_num.shape[1],), name='Numerical_Input')\n",
    "x_num = Dense(64, activation='relu')(input_num)\n",
    "x_num = Dropout(0.3)(x_num)\n",
    "x_num = Dense(32, activation='relu')(x_num)\n",
    "\n",
    "# LSTM 입력\n",
    "input_seq = Input(shape=(max_sequence_length,), name='Sequence_Input')\n",
    "x_seq = Embedding(input_dim=max_num_words, output_dim=embedding_dim)(input_seq)\n",
    "x_seq = LSTM(64, return_sequences=True)(x_seq)\n",
    "x_seq = LSTM(32, return_sequences=True)(x_seq)  # return_sequences=True로 수정\n",
    "\n",
    "# x_num을 3차원으로 변환\n",
    "x_num = Reshape((1, 32))(x_num)  # 32는 x_num의 마지막 차원 크기\n",
    "\n",
    "# 교차 주의 메커니즘\n",
    "attention_output = Attention()([x_seq, x_num])\n",
    "attention_output = LayerNormalization()(attention_output)\n",
    "\n",
    "# Flatten 및 최종 출력\n",
    "merged = Flatten()(attention_output)\n",
    "merged = Dense(64, activation='relu')(merged)\n",
    "merged = Dropout(0.3)(merged)\n",
    "output = Dense(y.shape[1], activation='softmax', name='Output')(merged)\n",
    "\n",
    "# 모델 정의\n",
    "model = Model(inputs=[input_num, input_seq], outputs=output)\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# 조기 종료 콜백\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# 6. 모델 컴파일 및 학습\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(\n",
    "    [X_train_num, X_train_seq], y_train,\n",
    "    validation_data=([X_val_num, X_val_seq], y_val),\n",
    "    epochs=20, batch_size=128,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# 7. 평가\n",
    "test_loss, test_accuracy = model.evaluate([X_test_num, X_test_seq], y_test)\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "# 모델 시각화 파일 저장 및 출력\n",
    "def visualize_model_in_colab(model, file_name=\"model_structure.png\"):\n",
    "    plot_model(\n",
    "        model,\n",
    "        to_file=file_name,\n",
    "        show_shapes=True,\n",
    "        show_layer_names=True\n",
    "    )\n",
    "    return Image(file_name)\n",
    "\n",
    "# 실행\n",
    "visualize_model_in_colab(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
